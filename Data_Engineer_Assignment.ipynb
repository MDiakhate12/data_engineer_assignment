{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDiakhate12/data_engineer_assignment/blob/main/Data_Engineer_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_VydxpbLYjI",
        "outputId": "56f834c2-946c-4c6d-e8fe-d11e9c05848f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.1.3\n",
            "  Using cached pyspark-3.1.3.tar.gz (214.0 MB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting py4j==0.10.9\n",
            "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "Using legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Running setup.py install for pyspark: started\n",
            "  Running setup.py install for pyspark: finished with status 'done'\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\mdiakhat\\OneDrive - Capgemini\\Documents\\data_engineering_assignment\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgvGrgBPpv3D",
        "outputId": "a287588b-8586-4202-cbee-e34f0691c9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-0.20.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\mdiakhat\\OneDrive - Capgemini\\Documents\\data_engineering_assignment\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARQMAc6bbFVS",
        "outputId": "ac7acc4a-db11-4426-cdc6-7fd44d71bcdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests\n",
            "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
            "     ---------------------------------------- 62.8/62.8 KB 1.7 MB/s eta 0:00:00\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
            "     -------------------------------------- 160.2/160.2 KB 1.9 MB/s eta 0:00:00\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
            "Successfully installed certifi-2022.6.15 charset-normalizer-2.0.12 idna-3.3 requests-2.28.0 urllib3-1.26.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\mdiakhat\\OneDrive - Capgemini\\Documents\\data_engineering_assignment\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "! pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0ZAEUUTL5m4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import * \n",
        "\n",
        "import os\n",
        "from os.path import splitext\n",
        "from os.path import join, dirname\n",
        "\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQo1iz2qv8o1"
      },
      "outputs": [],
      "source": [
        "INPUT_FORMAT = \"parquet\"\n",
        "\n",
        "DATE = \"2020-01\"\n",
        "\n",
        "BASE_DATASET_NAME = \"yellow_tripdata\"\n",
        "\n",
        "DATA_ROOT_PATH = join(dirname('__file__'), \"data\")\n",
        "\n",
        "FILENAME = f\"{BASE_DATASET_NAME}_{DATE}.{INPUT_FORMAT}\"\n",
        "\n",
        "BASE_URL = \"https://s3.amazonaws.com/nyc-tlc/trip+data\"\n",
        "\n",
        "DATASET_URL = f\"{BASE_URL}/{FILENAME}\"\n",
        "\n",
        "PARTITIONNED_FILENAME = f\"{splitext(FILENAME)[0]}_partitionned\"\n",
        "\n",
        "\n",
        "# List of maven coordinate jar packages \n",
        "JAR_PACKAGES = [\n",
        "                \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\", \n",
        "                \"com.microsoft.sqlserver:mssql-jdbc:10.2.0.jre8\",\n",
        "                ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-minYEM0Zp0",
        "outputId": "4d526e97-5fe9-4d06-9bb1-d7c6aac240f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INPUT_FORMAT: parquet\n",
            "DATE: 2020-01\n",
            "BASE_DATASET_NAME: yellow_tripdata\n",
            "FILENAME: yellow_tripdata_2020-01.parquet\n",
            "BASE_URL: https://s3.amazonaws.com/nyc-tlc/trip+data\n",
            "DATASET_URL: https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-01.parquet\n",
            "PARTITIONNED_FILENAME: yellow_tripdata_2020-01_partitionned\n",
            "JAR_PACKAGES: ['com.microsoft.azure:spark-mssql-connector_2.12:1.2.0', 'com.microsoft.sqlserver:mssql-jdbc:10.2.0.jre8']\n",
            "DATA_ROOT_PATH: data\n"
          ]
        }
      ],
      "source": [
        "print(\"INPUT_FORMAT:\", INPUT_FORMAT)\n",
        "print(\"DATE:\", DATE)\n",
        "print(\"BASE_DATASET_NAME:\", BASE_DATASET_NAME)\n",
        "print(\"FILENAME:\", FILENAME)\n",
        "print(\"BASE_URL:\", BASE_URL)\n",
        "print(\"DATASET_URL:\", DATASET_URL)\n",
        "print(\"PARTITIONNED_FILENAME:\", PARTITIONNED_FILENAME)\n",
        "print(\"JAR_PACKAGES:\", JAR_PACKAGES)\n",
        "print(\"DATA_ROOT_PATH:\", DATA_ROOT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcPu1iRfbKxF"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "  .builder \\\n",
        "  .master(\"local[*]\") \\\n",
        "  .appName(\"Data Engineer Assignment\") \\\n",
        "  .config(\"spark.jars.packages\", \",\".join(JAR_PACKAGES)) \\\n",
        "  .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lpp8JGfsvFm",
        "outputId": "e552d945-dc8d-4651-f004-d7a1bc5b294c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.1.3\n"
          ]
        }
      ],
      "source": [
        "# Showing the spark version\n",
        "\n",
        "print(spark.sparkContext.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQkOcYI7tVBq",
        "outputId": "411a38f3-fd94-4e62-b5b9-d7cd94d011ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector(spark://LFR028606.corp.capgemini.com:50061/jars/com.microsoft.azure_spark-mssql-connector_2.12-1.2.0.jar, spark://LFR028606.corp.capgemini.com:50061/jars/com.microsoft.sqlserver_mssql-jdbc-10.2.0.jre8.jar)\n"
          ]
        }
      ],
      "source": [
        "# Check if libraries has been correctly installed\n",
        "\n",
        "print(spark.sparkContext._jsc.sc().listJars())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNlr734kbFVd"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag3BG86JbFVd"
      },
      "outputs": [],
      "source": [
        "# ! rm -rf ${BASE_DATASET_NAME}*\n",
        "# ! ls ${BASE_DATASET_NAME}*\n",
        "# ! wget $DATASET_URL\n",
        "\n",
        "response = requests.get(DATASET_URL)\n",
        "\n",
        "with open(f\"{DATA_ROOT_PATH}/{FILENAME}\", \"wb\") as file:\n",
        "    file.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDvnWP4Ll-Gi"
      },
      "outputs": [],
      "source": [
        "df = spark.read.parquet(f\"{DATA_ROOT_PATH}/{FILENAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs6g95j-mdu1",
        "outputId": "9d99a0d9-6610-4d70-b822-32f92f2a49f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- VendorID: long (nullable = true)\n",
            " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
            " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
            " |-- passenger_count: double (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: double (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- PULocationID: long (nullable = true)\n",
            " |-- DOLocationID: long (nullable = true)\n",
            " |-- payment_type: long (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- congestion_surcharge: double (nullable = true)\n",
            " |-- airport_fee: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzDisJs7qLTr"
      },
      "source": [
        "# 1. Imagine that you receive data in single day batches, based on the field “tpep_dropoff_datetime”. Split the .csv file into multiple files based on this field. You should end up in roughly 30 files. Consider using a more efficient file format than csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7lxe5vVB9fM",
        "outputId": "50ad5fe1-2660-4136-d51f-cfe7278eee8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
            "|       1| 2020-01-01 01:28:15|  2020-01-01 01:33:03|            1.0|          1.2|       1.0|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|       null|\n",
            "|       1| 2020-01-01 01:35:39|  2020-01-01 01:43:04|            1.0|          1.2|       1.0|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|       null|\n",
            "|       1| 2020-01-01 01:47:41|  2020-01-01 01:53:52|            1.0|          0.6|       1.0|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|       null|\n",
            "|       1| 2020-01-01 01:55:23|  2020-01-01 02:00:14|            1.0|          0.8|       1.0|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|       null|\n",
            "|       2| 2020-01-01 01:01:58|  2020-01-01 01:04:16|            1.0|          0.0|       1.0|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|       null|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Overview of data \n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhtCqsV-2cNy",
        "outputId": "28af6fc3-68cc-49d4-fc06-bbb17e14bd4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2137286"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count the number of datetime values\n",
        "\n",
        "df.select(\"tpep_dropoff_datetime\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGN-LnjZxmBQ",
        "outputId": "ad35f80c-b1f6-46a3-f4d2-7bf97e813e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------+--------+-------------+-----------+\n",
            "|tpep_dropoff_datetime|VendorID|trip_distance|fare_amount|\n",
            "+---------------------+--------+-------------+-----------+\n",
            "|  2020-01-01 01:33:03|       1|          1.2|        6.0|\n",
            "|  2020-01-01 01:43:04|       1|          1.2|        7.0|\n",
            "|  2020-01-01 01:53:52|       1|          0.6|        6.0|\n",
            "|  2020-01-01 02:00:14|       1|          0.8|        5.5|\n",
            "|  2020-01-01 01:04:16|       2|          0.0|        3.5|\n",
            "+---------------------+--------+-------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quick check of some data values\n",
        "\n",
        "df.select(\"tpep_dropoff_datetime\", \"VendorID\", \"trip_distance\", \"fare_amount\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aZAvLA_ycyD",
        "outputId": "c9f39a5c-b541-4828-b0bb-7283825bd3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|dayofmonth|\n",
            "+----------+\n",
            "|         1|\n",
            "|         2|\n",
            "|         3|\n",
            "|         4|\n",
            "|         5|\n",
            "|         6|\n",
            "|         7|\n",
            "|         8|\n",
            "|         9|\n",
            "|        10|\n",
            "|        11|\n",
            "|        12|\n",
            "|        13|\n",
            "|        14|\n",
            "|        15|\n",
            "|        16|\n",
            "|        17|\n",
            "|        18|\n",
            "|        19|\n",
            "|        20|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Checking ordered distinct values of the \"tpep_dropoff_datetime\" field\n",
        "\n",
        "df \\\n",
        "  .select(dayofmonth(\"tpep_dropoff_datetime\").alias(\"dayofmonth\")) \\\n",
        "  .distinct() \\\n",
        "  .orderBy(\"dayofmonth\") \\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej5q-v0hhT3j",
        "outputId": "0f8653fd-66b5-420d-d188-6732f716b87f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Counting the number of distinct values into the \"tpep_dropoff_datetime\" field\n",
        "\n",
        "df.select(dayofmonth(\"tpep_dropoff_datetime\")).distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-aViDE8bFVk",
        "outputId": "8c08cda9-506f-434c-e0d3-e211ccd05d37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'data'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DATA_ROOT_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22DQMWNpp-Ps"
      },
      "outputs": [],
      "source": [
        "# Partition and save the date data by day for one month \n",
        "\n",
        "# We choose to save file into parquet format which is optimized for read \n",
        "\n",
        "df \\\n",
        "  .withColumn(\"dayofmonth\", dayofmonth(\"tpep_dropoff_datetime\")) \\\n",
        "  .write \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .partitionBy(\"dayofmonth\") \\\n",
        "  .parquet(f\"./data/yellow_tripdata_2020-01_partitionned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inlb_V1TiUeO",
        "outputId": "4f06e0c8-5281-4cf7-9156-be5389f2391e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 140\n",
            "drwxr-xr-x 33 root root 4096 Jun 21 11:34  .\n",
            "drwxr-xr-x  1 root root 4096 Jun 21 11:33  ..\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:33 'dayofmonth=1'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=10'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=11'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=12'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=13'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=14'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=15'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=16'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=17'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=18'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=19'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=2'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=20'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=21'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=22'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=23'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=24'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=25'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=26'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=27'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=28'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=29'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=3'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=30'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=31'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=4'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=5'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=6'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=7'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=8'\n",
            "drwxr-xr-x  2 root root 4096 Jun 21 11:34 'dayofmonth=9'\n",
            "-rw-r--r--  1 root root    0 Jun 21 11:34  _SUCCESS\n",
            "-rw-r--r--  1 root root    8 Jun 21 11:34  ._SUCCESS.crc\n"
          ]
        }
      ],
      "source": [
        "# Check if the data is correctly saved  \n",
        "\n",
        "# !ls -al $PARTITIONNED_FILENAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM-gmHfDrd6H"
      },
      "source": [
        "# 2. Your next task is to implement a process that loads all of the files into the database. It would be beneficial that the implementation is reusable, i.e.can be used for the future data files as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txfrt68up7Uy",
        "outputId": "9f88bd36-2375-4101-f75b-3b7f5b8a3908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 159 Jun 21 09:53 .env\n"
          ]
        }
      ],
      "source": [
        "# Showing the dotenv file created with Azure SQL DB credentials\n",
        "\n",
        "!ls -al .env*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7F2G5uxqzao",
        "outputId": "5cc5ec98-e180-4356-ab01-1759ffd45ccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".env\n"
          ]
        }
      ],
      "source": [
        "dotenv_path = join(dirname('__file__'), '.env')\n",
        "print(dotenv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_oQ14UfrHYL",
        "outputId": "726742cf-e379-40ef-e6a6-27c782d9bab1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv(dotenv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekXRNWBcpLhc"
      },
      "outputs": [],
      "source": [
        "# Azure SQL DB Database Credential\n",
        "HOSTNAME = os.environ.get(\"HOSTNAME\")\n",
        "USER = os.environ.get(\"USER\")\n",
        "PASSWORD = os.environ.get(\"PASSWORD\")\n",
        "DATABASE = os.environ.get(\"DATABASE\")\n",
        "PORT = os.environ.get(\"PORT\")\n",
        "SERVER = os.environ.get(\"SERVER\")\n",
        "\n",
        "# SQLServer Driver\n",
        "DRIVER = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" \n",
        "\n",
        "# Azure SQL Connector JDBC Format\n",
        "SQL_SERVER_SPARK_JDBC_FORMART = \"com.microsoft.sqlserver.jdbc.spark\"\n",
        "\n",
        "# Database Connection String\n",
        "URL =  f\"jdbc:sqlserver://{HOSTNAME}:{PORT};databaseName={DATABASE};\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUh8U-yvwe17"
      },
      "outputs": [],
      "source": [
        "reader = spark.read.format(SQL_SERVER_SPARK_JDBC_FORMART) \\\n",
        "    .option(\"url\", URL) \\\n",
        "    .option(\"user\", USER) \\\n",
        "    .option(\"driver\", DRIVER) \\\n",
        "    .option(\"password\", PASSWORD) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxhp0076wzFe"
      },
      "outputs": [],
      "source": [
        "df_partitionned = spark.read.parquet(PARTITIONNED_FILENAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB7K1TOTzcRH",
        "outputId": "53238ca0-ee64-4c36-e437-69a6da9da33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|dayofmonth|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----------+\n",
            "|       2| 2020-01-16 23:34:00|  2020-01-17 00:08:00|           null|         14.1|      null|              null|         136|          90|           0|      51.05| 2.75|    0.5|       0.0|         0.0|                  0.3|        54.6|                null|       null|        17|\n",
            "|       2| 2020-01-16 00:25:38|  2020-01-17 00:12:30|            6.0|         8.23|       1.0|                 N|         164|         116|           1|       30.5|  0.5|    0.5|      6.86|         0.0|                  0.3|       41.16|                 2.5|       null|        17|\n",
            "|       2| 2020-01-16 23:44:00|  2020-01-17 00:32:00|           null|         33.1|      null|              null|          66|          86|           0|      51.04| 2.75|    0.5|       0.0|         0.0|                  0.3|       54.59|                null|       null|        17|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_partitionned.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqP8li0qptJD"
      },
      "outputs": [],
      "source": [
        "df_partitionned.write.format(SQL_SERVER_SPARK_JDBC_FORMART) \\\n",
        "    .option(\"mode\", \"overwrite\") \\\n",
        "    .option(\"url\", URL) \\\n",
        "    .option(\"user\", USER) \\\n",
        "    .option(\"driver\", DRIVER) \\\n",
        "    .option(\"password\", PASSWORD) \\\n",
        "    .option(\"dbtable\", f\"{BASE_DATASET_NAME}_{DATE}\") \\\n",
        "    .save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_V8yum6r16c"
      },
      "source": [
        "# 3. Test the quality of the data. What metrics would you use to monitor the quality of the data? Are there any anomalous or suspicious data points in the data? We would like to collect data quality metrics of processed data each time it is loaded into the database. Could you implement this functionality and store such metrics in a separate database table or any other place that you seem fit?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C81Urm6rsDUH"
      },
      "source": [
        "# 4. Model the dataset to represent a fact-dimension type of schema. Draw the entity relationship diagram of your model and transform the data to fit it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZHq3qtnsGhK"
      },
      "source": [
        "# 5. Can you identify the weak points of the proposed solution? If you had more time, what parts of your solution would you improve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6Y2XuxbFVo"
      },
      "source": [
        "# Weak points and Imporvements\n",
        "\n",
        "1.  The code should be written with functions and parameterized to be more reusable and easier to test (unit tests and integration tests)\n",
        "2.  We should use a defined schema for the data instead of inferring it: this enables the data loading to be faster and more robust and also prevent any type issue\n",
        "3.  We should use try-catch blocks to prevent our data pipeline from crashing by catching the corresponding exceptions \n",
        "4.  The data must be ingested in a storage in the cloud or locally in a more secure data storage and decoupled from the code in order to prevent any data loss\n",
        "5.  We can decouple data ingestion and data transformation to have better modularity\n",
        "6.  By using the decoupling mehod we can use Kafka to ingest data in an event based way\n",
        "7.  We can use KNative to deploy our pipeline as a serverless container. In this way, the container can be configured to be triggered only when new data arrives and then free ressources after that. The requirement for that is to have a Kubernetes cluster running either in a cloud provider or locally\n",
        "8.  After my experience with fact-dimension model, I should instead go for a single large table such as Big Query. This makes requests easier without the need to manage many foreign keys which require join requests.\n",
        "9.  This code is just a notebook for demonstration purposes. But the real code would be packaged in a python file and submitted via spark-submit and/or dockerized\n",
        "10. To test the quality of the data, we can refer to thoses five metrics: acccuracy (correct number scale and precisions), validity (correct data types and format), completeness (null values, zero values), uniqueness (correct integrity constrains) and conformity with the data type and business requirements\n",
        "11. We can use the pandas-profiling library to profile the data and get some useful metrics and prometheus with grafana to query and show more complex metrics\n",
        "12. With simple SQL requests we can create tables and ingest data and so some simple checks \n",
        "13. We can automate all the pipeline either by using simple cron jobs or by going more robust by using apache airflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppLpBuHhbFVo"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Data_Engineer_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "db9a97e2bb779b42e546f85bac47b2ee1ff025510c52ef850324a9e620bee06f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}